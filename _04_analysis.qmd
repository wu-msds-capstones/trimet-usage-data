```{r echo=FALSE, include=FALSE} 
library(tidyverse)
library(cowplot)
```

```{r, echo=F, include=FALSE}
stop_features = read_csv('data/stop_features.csv')

stop_features_plot = stop_features %>% mutate(
  stop_type = ifelse(stop_type == 'MAX Station', "MAX Station", 'Bus & WES Stations'), 
  day = as.factor(day)
  )

```


# Methods

## Data Collection and Manipulation 

### PDF Data Extraction and Webscraping 

Data used for this project was available publicly through TriMet's website, though in several different forms. This required a number of ingestion strategies. 

Usage data was provided in a number of quarterly report PDFs [@trimet_census] on the TriMet website [@usage_access], which were then ingested and processed to extract the data. Each page in the PDF represented a single route, on a single day, going a single direction. It listed each station along the path, with its usage numbers over the entirety of Spring 2025. Trimet only releases their usage numbers for Spring and Fall, so this is effectively half a year’s worth of data. The PDFs were originally partitioned into Weekday, Saturday, and Sunday documents, but we would eventually combine the Saturday and Sunday schedules into a single “weekend” category.

Information about TriMet schedules was, however, slightly more complicated. Schedules as posted on the TriMet website under their Schedules tab only listed a small subset of the station stops that are present in the usage reports. Thankfully, TriMet also has their schedules listed by station, searchable by station ID number, such as the list at [@trimet_stop], which can be accessed from [@stop_page]. This meant that once a list of IDs had been collected from the PDF reports, those IDs could be cross-referenced with the station schedules to obtain the raw schedule data for our many stops of interest, rather than a tenth of them. 

### Database Construction 

The data was organized into four tables: stations, routes, schedule, and usage. Even though the schedules and usage were already separated, we needed to ensure that the route names, station names, and station ids all matched between files. For these reasons, we used an R script to prepare the data for ingestion into the Postgres database. The output was another four csv files that would each correspond to one of the database tables and could be copied in directly. Below (@fig-erd) is an Entity Relationship Diagram showing our database organization:

![Entity Relationship Diagram for the database constructed in this project.](erd.png){fig-erd}

Stations and routes each have their own table, and are connected by two junction tables, schedule and usage. Each route is defined as a series of stations going a specific direction on a specific day. The usage table gives a total usage statistic for a specific station on a specific route. Alternatively, you can use the usage_id to select a station-route combination. This is useful within the schedule table, where the same route_id and station_id can have multiple arrival times in a schedule. This ensures third normal form because each table has a primary key and each non-key attribute depends solely on the key.

### Feature Engineering 

Each station has a large number of arrival times from a varying number of routes. However, arrival time alone makes for a poor feature. In pursuit of better features, a number of values were engineered from the schedule data for each station stop: the number of unique routes that stop at the station, the earliest and latest times the station is used, the number of vehicles that stop at the station over the course of the day, and the average number of minutes separating arrivals at the station. These values are all calculated for each station on both weekdays and weekend days. This was done overall by selecting the arrivals at each station on a given day and counting or otherwise manipulating the data into the features of interest. The total usage number sums for each station were also calculated at this stage, since usage information was originally provided at the route level for each station. 

## Ethics in Action 

Consider how long you would be willing to stand at a transit station waiting if you watched the previous vehicle stop at the station from across the street and leave without you on it. It's likely you picked about 20 minutes. In this analysis, we engineered a variable that helps with this consideration: the average time between arrivals at each station. We created boxplots of this variable, separated by station type in the TriMet system. This variable was calculated by collecting all the arrivals at a station and averaging the differences between them, so stops that are only active during a certain time of day are penalized by that. For example, the WES trains only run during rush hour, with a large gap in the middle of the day, so there is one artificially large value. By combining the WES data with the bus data, this discrepancy has a minimal effect on the data. Overall, however, most routes run consistently throughout the day and the values shown should be reflective of standing at a station waiting for a transit line to arrive as scheduled. 

## Statistical Modeling 

This project used Poisson regression to model station usage as a function of the engineered features. The log of the usage values had to be used, as there are a number of TriMet routes (specifically, the MAX lines) that have usage numbers that could be considered outliers, while still being important data points in our analysis.

It’s possible that Poisson regression is not the correct statistical model to use for our data, since one of its assumptions is equidispersion. Due to the strength of the MAX lines, our data is heavily right skewed with a wildly differing mean and variance.

## Machine Learning 

Because of the left-skewed distributions of the usage values in this dataset (seen in @fig-usage-dist), the usage values had to be log-transformed to act as good target values. The resulting distribution of this transformation can be seen in @fig-log-dist. There were 1,571 stations that had usage values of 0 and thus produced invalid log-transformed values. These stations were excluded from the regressions undertaken in this project. 

```{r, echo=F, message=F}
#| label: fig-log-dist
#| fig-cap: 'Figure X: Distributions of Log-transformed usage values, by station type. These were the values used as targets for the regressions in this project. 1,571 stations had invalid log-transformed usage values due to an original usage value of 0, and are excluded from this plot.'
ggplot(data=stop_features_plot, aes(x=log(total_usage), fill=stop_type)) + 
  geom_histogram() +
  theme_bw()+ 
  labs(
    title='Natural Log of Station Usage Distribtions, by Station Type', 
    legend.title='Station Type', 
    x='log(Total Usage)', 
    y = 'Number of Stations',
    caption='Data from Trimet', 
    fill='Station Type'
  )+
  scale_fill_manual(values=c('cornflowerblue', 'orange', 'blue3'))
```

This project used regression models in an attempt to predict station usage numbers from the engineered features described earlier in this paper. The inputs given to each model as independent variables were the number of unique routes that stop at the station, the earliest and latest times the station is used, and the average number of minutes separating arrivals at the station, as well as a Boolean value for day of the week and a Boolean value indicating whether each station is a MAX station. The number of vehicles that stop at the station was not used, as it had a correlation coefficient of 0.677 with the number of routes, which indicated colinearity. These inputs were used to predict the log-transformed usage values of each station. 

The models investigated increased in complexity from standard linear regression through a variety of Support Vector Machine kernels to random forest regression and a multi-layer perceptron. The random forest model was determined to have the best performance on this dataset, and as such will be the main model discussed in this report. The final, best-performing random forest had 100 trees and a tune length of 3, and was trained using 3-fold cross-validation. 